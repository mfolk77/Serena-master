program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3500.14.1"}, {"coremlc-version", "3500.32.1"}, {"coremltools-component-torch", "2.9.0"}, {"coremltools-source-dialect", "TorchScript"}, {"coremltools-version", "8.3.0"}})]
{
    func main<ios16>(tensor<int32, [1, 128]> attention_mask, tensor<int32, [1, 128]> input_ids) {
            tensor<int32, []> var_48_axis_0 = const()[name = tensor<string, []>("op_48_axis_0"), val = tensor<int32, []>(0)];
            tensor<int32, []> var_48_batch_dims_0 = const()[name = tensor<string, []>("op_48_batch_dims_0"), val = tensor<int32, []>(0)];
            tensor<fp16, [30522, 384]> model_embeddings_word_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("model_embeddings_word_embeddings_weight_to_fp16"), val = tensor<fp16, [30522, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64)))];
            tensor<fp16, [1, 128, 384]> var_48_cast_fp16 = gather(axis = var_48_axis_0, batch_dims = var_48_batch_dims_0, indices = input_ids, x = model_embeddings_word_embeddings_weight_to_fp16)[name = tensor<string, []>("op_48_cast_fp16")];
            tensor<fp16, [1, 128, 384]> var_50_to_fp16 = const()[name = tensor<string, []>("op_50_to_fp16"), val = tensor<fp16, [1, 128, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23441024)))];
            tensor<fp16, [1, 128, 384]> var_51_cast_fp16 = add(x = var_48_cast_fp16, y = var_50_to_fp16)[name = tensor<string, []>("op_51_cast_fp16")];
            tensor<fp16, [1, 128, 384]> var_53_to_fp16 = const()[name = tensor<string, []>("op_53_to_fp16"), val = tensor<fp16, [1, 128, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23539392)))];
            tensor<fp16, [1, 128, 384]> input_5_cast_fp16 = add(x = var_51_cast_fp16, y = var_53_to_fp16)[name = tensor<string, []>("input_5_cast_fp16")];
            tensor<int32, [1]> input_7_axes_0 = const()[name = tensor<string, []>("input_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_embeddings_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_embeddings_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23637760)))];
            tensor<fp16, [384]> model_embeddings_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_embeddings_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23638592)))];
            tensor<fp16, []> var_18_to_fp16 = const()[name = tensor<string, []>("op_18_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 128, 384]> input_7_cast_fp16 = layer_norm(axes = input_7_axes_0, beta = model_embeddings_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_embeddings_LayerNorm_weight_to_fp16, x = input_5_cast_fp16)[name = tensor<string, []>("input_7_cast_fp16")];
            tensor<int32, [1]> var_63_axes_0 = const()[name = tensor<string, []>("op_63_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [1, 1, 128]> var_63 = expand_dims(axes = var_63_axes_0, x = attention_mask)[name = tensor<string, []>("op_63")];
            tensor<int32, [1]> var_64_axes_0 = const()[name = tensor<string, []>("op_64_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<int32, [1, 1, 1, 128]> var_64 = expand_dims(axes = var_64_axes_0, x = var_63)[name = tensor<string, []>("op_64")];
            tensor<int32, [4]> var_67_reps_0 = const()[name = tensor<string, []>("op_67_reps_0"), val = tensor<int32, [4]>([1, 1, 128, 1])];
            tensor<int32, [1, 1, 128, 128]> var_67 = tile(reps = var_67_reps_0, x = var_64)[name = tensor<string, []>("op_67")];
            tensor<fp16, []> const_5_to_fp16 = const()[name = tensor<string, []>("const_5_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<string, []> cast_3_to_fp16_dtype_0 = const()[name = tensor<string, []>("cast_3_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 1, 128, 128]> var_67_to_fp16 = cast(dtype = cast_3_to_fp16_dtype_0, x = var_67)[name = tensor<string, []>("cast_31")];
            tensor<fp16, [1, 1, 128, 128]> var_71_cast_fp16 = sub(x = const_5_to_fp16, y = var_67_to_fp16)[name = tensor<string, []>("op_71_cast_fp16")];
            tensor<string, []> cast_4_dtype_0 = const()[name = tensor<string, []>("cast_4_dtype_0"), val = tensor<string, []>("bool")];
            tensor<fp16, []> var_8_to_fp16 = const()[name = tensor<string, []>("op_8_to_fp16"), val = tensor<fp16, []>(-inf)];
            tensor<bool, [1, 1, 128, 128]> var_71_cast_fp16_to_bool = cast(dtype = cast_4_dtype_0, x = var_71_cast_fp16)[name = tensor<string, []>("cast_30")];
            tensor<fp16, [1, 1, 128, 128]> var_73_cast_fp16 = select(a = var_8_to_fp16, b = var_71_cast_fp16, cond = var_71_cast_fp16_to_bool)[name = tensor<string, []>("op_73_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23639424)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23934400)))];
            tensor<fp16, [1, 128, 384]> linear_0_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_query_bias_to_fp16, weight = model_encoder_layer_0_attention_self_query_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_0_cast_fp16")];
            tensor<int32, [4]> var_99 = const()[name = tensor<string, []>("op_99"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_100_cast_fp16 = reshape(shape = var_99, x = linear_0_cast_fp16)[name = tensor<string, []>("op_100_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23935232)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24230208)))];
            tensor<fp16, [1, 128, 384]> linear_1_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_key_bias_to_fp16, weight = model_encoder_layer_0_attention_self_key_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_1_cast_fp16")];
            tensor<int32, [4]> var_105 = const()[name = tensor<string, []>("op_105"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_106_cast_fp16 = reshape(shape = var_105, x = linear_1_cast_fp16)[name = tensor<string, []>("op_106_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24231040)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24526016)))];
            tensor<fp16, [1, 128, 384]> linear_2_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_value_bias_to_fp16, weight = model_encoder_layer_0_attention_self_value_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_2_cast_fp16")];
            tensor<int32, [4]> var_111 = const()[name = tensor<string, []>("op_111"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_112_cast_fp16 = reshape(shape = var_111, x = linear_2_cast_fp16)[name = tensor<string, []>("op_112_cast_fp16")];
            tensor<int32, [4]> var_113_perm_0 = const()[name = tensor<string, []>("op_113_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_0_y_0_to_fp16 = const()[name = tensor<string, []>("mul_0_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 128, 12, 32]> mul_0_cast_fp16 = mul(x = var_100_cast_fp16, y = mul_0_y_0_to_fp16)[name = tensor<string, []>("mul_0_cast_fp16")];
            tensor<bool, []> matmul_0_transpose_y_0 = const()[name = tensor<string, []>("matmul_0_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_0_transpose_x_0 = const()[name = tensor<string, []>("matmul_0_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_24_perm_0 = const()[name = tensor<string, []>("transpose_24_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_25_perm_0 = const()[name = tensor<string, []>("transpose_25_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 128, 32]> transpose_25 = transpose(perm = transpose_25_perm_0, x = var_106_cast_fp16)[name = tensor<string, []>("transpose_57")];
            tensor<fp16, [1, 12, 128, 32]> transpose_24 = transpose(perm = transpose_24_perm_0, x = mul_0_cast_fp16)[name = tensor<string, []>("transpose_58")];
            tensor<fp16, [1, 12, 128, 128]> matmul_0_cast_fp16 = matmul(transpose_x = matmul_0_transpose_x_0, transpose_y = matmul_0_transpose_y_0, x = transpose_24, y = transpose_25)[name = tensor<string, []>("matmul_0_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_0_cast_fp16 = add(x = matmul_0_cast_fp16, y = var_73_cast_fp16)[name = tensor<string, []>("add_0_cast_fp16")];
            tensor<int32, []> softmax_0_axis_0 = const()[name = tensor<string, []>("softmax_0_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_0_cast_fp16 = softmax(axis = softmax_0_axis_0, x = add_0_cast_fp16)[name = tensor<string, []>("softmax_0_cast_fp16")];
            tensor<bool, []> var_114_transpose_x_0 = const()[name = tensor<string, []>("op_114_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> var_114_transpose_y_0 = const()[name = tensor<string, []>("op_114_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> var_113_cast_fp16 = transpose(perm = var_113_perm_0, x = var_112_cast_fp16)[name = tensor<string, []>("transpose_59")];
            tensor<fp16, [1, 12, 128, 32]> var_114_cast_fp16 = matmul(transpose_x = var_114_transpose_x_0, transpose_y = var_114_transpose_y_0, x = softmax_0_cast_fp16, y = var_113_cast_fp16)[name = tensor<string, []>("op_114_cast_fp16")];
            tensor<int32, [4]> var_115_perm_0 = const()[name = tensor<string, []>("op_115_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_116 = const()[name = tensor<string, []>("op_116"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> var_115_cast_fp16 = transpose(perm = var_115_perm_0, x = var_114_cast_fp16)[name = tensor<string, []>("transpose_56")];
            tensor<fp16, [1, 128, 384]> var_117_cast_fp16 = reshape(shape = var_116, x = var_115_cast_fp16)[name = tensor<string, []>("op_117_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24526848)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24821824)))];
            tensor<fp16, [1, 128, 384]> linear_3_cast_fp16 = linear(bias = model_encoder_layer_0_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_0_attention_output_dense_weight_to_fp16, x = var_117_cast_fp16)[name = tensor<string, []>("linear_3_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_11_cast_fp16 = add(x = linear_3_cast_fp16, y = input_7_cast_fp16)[name = tensor<string, []>("input_11_cast_fp16")];
            tensor<int32, [1]> var_128_axes_0 = const()[name = tensor<string, []>("op_128_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24822656)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24823488)))];
            tensor<fp16, [1, 128, 384]> var_128_cast_fp16 = layer_norm(axes = var_128_axes_0, beta = model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16, x = input_11_cast_fp16)[name = tensor<string, []>("op_128_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_0_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24824320)))];
            tensor<fp16, [1536]> model_encoder_layer_0_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26004032)))];
            tensor<fp16, [1, 128, 1536]> linear_4_cast_fp16 = linear(bias = model_encoder_layer_0_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_0_intermediate_dense_weight_to_fp16, x = var_128_cast_fp16)[name = tensor<string, []>("linear_4_cast_fp16")];
            tensor<string, []> var_133_mode_0 = const()[name = tensor<string, []>("op_133_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> var_133_cast_fp16 = gelu(mode = var_133_mode_0, x = linear_4_cast_fp16)[name = tensor<string, []>("op_133_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_0_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26007168)))];
            tensor<fp16, [384]> model_encoder_layer_0_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27186880)))];
            tensor<fp16, [1, 128, 384]> linear_5_cast_fp16 = linear(bias = model_encoder_layer_0_output_dense_bias_to_fp16, weight = model_encoder_layer_0_output_dense_weight_to_fp16, x = var_133_cast_fp16)[name = tensor<string, []>("linear_5_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_15_cast_fp16 = add(x = linear_5_cast_fp16, y = var_128_cast_fp16)[name = tensor<string, []>("input_15_cast_fp16")];
            tensor<int32, [1]> var_144_axes_0 = const()[name = tensor<string, []>("op_144_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27187712)))];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27188544)))];
            tensor<fp16, [1, 128, 384]> var_144_cast_fp16 = layer_norm(axes = var_144_axes_0, beta = model_encoder_layer_0_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_0_output_LayerNorm_weight_to_fp16, x = input_15_cast_fp16)[name = tensor<string, []>("op_144_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27189376)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27484352)))];
            tensor<fp16, [1, 128, 384]> linear_6_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_query_bias_to_fp16, weight = model_encoder_layer_1_attention_self_query_weight_to_fp16, x = var_144_cast_fp16)[name = tensor<string, []>("linear_6_cast_fp16")];
            tensor<int32, [4]> var_158 = const()[name = tensor<string, []>("op_158"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_159_cast_fp16 = reshape(shape = var_158, x = linear_6_cast_fp16)[name = tensor<string, []>("op_159_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27485184)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27780160)))];
            tensor<fp16, [1, 128, 384]> linear_7_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_key_bias_to_fp16, weight = model_encoder_layer_1_attention_self_key_weight_to_fp16, x = var_144_cast_fp16)[name = tensor<string, []>("linear_7_cast_fp16")];
            tensor<int32, [4]> var_164 = const()[name = tensor<string, []>("op_164"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_165_cast_fp16 = reshape(shape = var_164, x = linear_7_cast_fp16)[name = tensor<string, []>("op_165_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27780992)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28075968)))];
            tensor<fp16, [1, 128, 384]> linear_8_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_value_bias_to_fp16, weight = model_encoder_layer_1_attention_self_value_weight_to_fp16, x = var_144_cast_fp16)[name = tensor<string, []>("linear_8_cast_fp16")];
            tensor<int32, [4]> var_170 = const()[name = tensor<string, []>("op_170"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_171_cast_fp16 = reshape(shape = var_170, x = linear_8_cast_fp16)[name = tensor<string, []>("op_171_cast_fp16")];
            tensor<int32, [4]> var_172_perm_0 = const()[name = tensor<string, []>("op_172_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_1_y_0_to_fp16 = const()[name = tensor<string, []>("mul_1_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 128, 12, 32]> mul_1_cast_fp16 = mul(x = var_159_cast_fp16, y = mul_1_y_0_to_fp16)[name = tensor<string, []>("mul_1_cast_fp16")];
            tensor<bool, []> matmul_1_transpose_y_0 = const()[name = tensor<string, []>("matmul_1_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_1_transpose_x_0 = const()[name = tensor<string, []>("matmul_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_26_perm_0 = const()[name = tensor<string, []>("transpose_26_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_27_perm_0 = const()[name = tensor<string, []>("transpose_27_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 128, 32]> transpose_27 = transpose(perm = transpose_27_perm_0, x = var_165_cast_fp16)[name = tensor<string, []>("transpose_53")];
            tensor<fp16, [1, 12, 128, 32]> transpose_26 = transpose(perm = transpose_26_perm_0, x = mul_1_cast_fp16)[name = tensor<string, []>("transpose_54")];
            tensor<fp16, [1, 12, 128, 128]> matmul_1_cast_fp16 = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = transpose_26, y = transpose_27)[name = tensor<string, []>("matmul_1_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_1_cast_fp16 = add(x = matmul_1_cast_fp16, y = var_73_cast_fp16)[name = tensor<string, []>("add_1_cast_fp16")];
            tensor<int32, []> softmax_1_axis_0 = const()[name = tensor<string, []>("softmax_1_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_1_cast_fp16 = softmax(axis = softmax_1_axis_0, x = add_1_cast_fp16)[name = tensor<string, []>("softmax_1_cast_fp16")];
            tensor<bool, []> var_173_transpose_x_0 = const()[name = tensor<string, []>("op_173_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> var_173_transpose_y_0 = const()[name = tensor<string, []>("op_173_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> var_172_cast_fp16 = transpose(perm = var_172_perm_0, x = var_171_cast_fp16)[name = tensor<string, []>("transpose_55")];
            tensor<fp16, [1, 12, 128, 32]> var_173_cast_fp16 = matmul(transpose_x = var_173_transpose_x_0, transpose_y = var_173_transpose_y_0, x = softmax_1_cast_fp16, y = var_172_cast_fp16)[name = tensor<string, []>("op_173_cast_fp16")];
            tensor<int32, [4]> var_174_perm_0 = const()[name = tensor<string, []>("op_174_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_175 = const()[name = tensor<string, []>("op_175"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> var_174_cast_fp16 = transpose(perm = var_174_perm_0, x = var_173_cast_fp16)[name = tensor<string, []>("transpose_52")];
            tensor<fp16, [1, 128, 384]> var_176_cast_fp16 = reshape(shape = var_175, x = var_174_cast_fp16)[name = tensor<string, []>("op_176_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28076800)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28371776)))];
            tensor<fp16, [1, 128, 384]> linear_9_cast_fp16 = linear(bias = model_encoder_layer_1_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_1_attention_output_dense_weight_to_fp16, x = var_176_cast_fp16)[name = tensor<string, []>("linear_9_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_19_cast_fp16 = add(x = linear_9_cast_fp16, y = var_144_cast_fp16)[name = tensor<string, []>("input_19_cast_fp16")];
            tensor<int32, [1]> var_187_axes_0 = const()[name = tensor<string, []>("op_187_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28372608)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28373440)))];
            tensor<fp16, [1, 128, 384]> var_187_cast_fp16 = layer_norm(axes = var_187_axes_0, beta = model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16, x = input_19_cast_fp16)[name = tensor<string, []>("op_187_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_1_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28374272)))];
            tensor<fp16, [1536]> model_encoder_layer_1_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29553984)))];
            tensor<fp16, [1, 128, 1536]> linear_10_cast_fp16 = linear(bias = model_encoder_layer_1_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_1_intermediate_dense_weight_to_fp16, x = var_187_cast_fp16)[name = tensor<string, []>("linear_10_cast_fp16")];
            tensor<string, []> var_192_mode_0 = const()[name = tensor<string, []>("op_192_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> var_192_cast_fp16 = gelu(mode = var_192_mode_0, x = linear_10_cast_fp16)[name = tensor<string, []>("op_192_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_1_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29557120)))];
            tensor<fp16, [384]> model_encoder_layer_1_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30736832)))];
            tensor<fp16, [1, 128, 384]> linear_11_cast_fp16 = linear(bias = model_encoder_layer_1_output_dense_bias_to_fp16, weight = model_encoder_layer_1_output_dense_weight_to_fp16, x = var_192_cast_fp16)[name = tensor<string, []>("linear_11_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_23_cast_fp16 = add(x = linear_11_cast_fp16, y = var_187_cast_fp16)[name = tensor<string, []>("input_23_cast_fp16")];
            tensor<int32, [1]> var_203_axes_0 = const()[name = tensor<string, []>("op_203_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30737664)))];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30738496)))];
            tensor<fp16, [1, 128, 384]> var_203_cast_fp16 = layer_norm(axes = var_203_axes_0, beta = model_encoder_layer_1_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_1_output_LayerNorm_weight_to_fp16, x = input_23_cast_fp16)[name = tensor<string, []>("op_203_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30739328)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31034304)))];
            tensor<fp16, [1, 128, 384]> linear_12_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_query_bias_to_fp16, weight = model_encoder_layer_2_attention_self_query_weight_to_fp16, x = var_203_cast_fp16)[name = tensor<string, []>("linear_12_cast_fp16")];
            tensor<int32, [4]> var_217 = const()[name = tensor<string, []>("op_217"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_218_cast_fp16 = reshape(shape = var_217, x = linear_12_cast_fp16)[name = tensor<string, []>("op_218_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31035136)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31330112)))];
            tensor<fp16, [1, 128, 384]> linear_13_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_key_bias_to_fp16, weight = model_encoder_layer_2_attention_self_key_weight_to_fp16, x = var_203_cast_fp16)[name = tensor<string, []>("linear_13_cast_fp16")];
            tensor<int32, [4]> var_223 = const()[name = tensor<string, []>("op_223"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_224_cast_fp16 = reshape(shape = var_223, x = linear_13_cast_fp16)[name = tensor<string, []>("op_224_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31330944)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31625920)))];
            tensor<fp16, [1, 128, 384]> linear_14_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_value_bias_to_fp16, weight = model_encoder_layer_2_attention_self_value_weight_to_fp16, x = var_203_cast_fp16)[name = tensor<string, []>("linear_14_cast_fp16")];
            tensor<int32, [4]> var_229 = const()[name = tensor<string, []>("op_229"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_230_cast_fp16 = reshape(shape = var_229, x = linear_14_cast_fp16)[name = tensor<string, []>("op_230_cast_fp16")];
            tensor<int32, [4]> var_231_perm_0 = const()[name = tensor<string, []>("op_231_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_2_y_0_to_fp16 = const()[name = tensor<string, []>("mul_2_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 128, 12, 32]> mul_2_cast_fp16 = mul(x = var_218_cast_fp16, y = mul_2_y_0_to_fp16)[name = tensor<string, []>("mul_2_cast_fp16")];
            tensor<bool, []> matmul_2_transpose_y_0 = const()[name = tensor<string, []>("matmul_2_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_2_transpose_x_0 = const()[name = tensor<string, []>("matmul_2_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_28_perm_0 = const()[name = tensor<string, []>("transpose_28_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_29_perm_0 = const()[name = tensor<string, []>("transpose_29_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 128, 32]> transpose_29 = transpose(perm = transpose_29_perm_0, x = var_224_cast_fp16)[name = tensor<string, []>("transpose_49")];
            tensor<fp16, [1, 12, 128, 32]> transpose_28 = transpose(perm = transpose_28_perm_0, x = mul_2_cast_fp16)[name = tensor<string, []>("transpose_50")];
            tensor<fp16, [1, 12, 128, 128]> matmul_2_cast_fp16 = matmul(transpose_x = matmul_2_transpose_x_0, transpose_y = matmul_2_transpose_y_0, x = transpose_28, y = transpose_29)[name = tensor<string, []>("matmul_2_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_2_cast_fp16 = add(x = matmul_2_cast_fp16, y = var_73_cast_fp16)[name = tensor<string, []>("add_2_cast_fp16")];
            tensor<int32, []> softmax_2_axis_0 = const()[name = tensor<string, []>("softmax_2_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_2_cast_fp16 = softmax(axis = softmax_2_axis_0, x = add_2_cast_fp16)[name = tensor<string, []>("softmax_2_cast_fp16")];
            tensor<bool, []> var_232_transpose_x_0 = const()[name = tensor<string, []>("op_232_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> var_232_transpose_y_0 = const()[name = tensor<string, []>("op_232_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> var_231_cast_fp16 = transpose(perm = var_231_perm_0, x = var_230_cast_fp16)[name = tensor<string, []>("transpose_51")];
            tensor<fp16, [1, 12, 128, 32]> var_232_cast_fp16 = matmul(transpose_x = var_232_transpose_x_0, transpose_y = var_232_transpose_y_0, x = softmax_2_cast_fp16, y = var_231_cast_fp16)[name = tensor<string, []>("op_232_cast_fp16")];
            tensor<int32, [4]> var_233_perm_0 = const()[name = tensor<string, []>("op_233_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_234 = const()[name = tensor<string, []>("op_234"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> var_233_cast_fp16 = transpose(perm = var_233_perm_0, x = var_232_cast_fp16)[name = tensor<string, []>("transpose_48")];
            tensor<fp16, [1, 128, 384]> var_235_cast_fp16 = reshape(shape = var_234, x = var_233_cast_fp16)[name = tensor<string, []>("op_235_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31626752)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31921728)))];
            tensor<fp16, [1, 128, 384]> linear_15_cast_fp16 = linear(bias = model_encoder_layer_2_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_2_attention_output_dense_weight_to_fp16, x = var_235_cast_fp16)[name = tensor<string, []>("linear_15_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_27_cast_fp16 = add(x = linear_15_cast_fp16, y = var_203_cast_fp16)[name = tensor<string, []>("input_27_cast_fp16")];
            tensor<int32, [1]> var_246_axes_0 = const()[name = tensor<string, []>("op_246_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31922560)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31923392)))];
            tensor<fp16, [1, 128, 384]> var_246_cast_fp16 = layer_norm(axes = var_246_axes_0, beta = model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16, x = input_27_cast_fp16)[name = tensor<string, []>("op_246_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_2_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31924224)))];
            tensor<fp16, [1536]> model_encoder_layer_2_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33103936)))];
            tensor<fp16, [1, 128, 1536]> linear_16_cast_fp16 = linear(bias = model_encoder_layer_2_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_2_intermediate_dense_weight_to_fp16, x = var_246_cast_fp16)[name = tensor<string, []>("linear_16_cast_fp16")];
            tensor<string, []> var_251_mode_0 = const()[name = tensor<string, []>("op_251_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> var_251_cast_fp16 = gelu(mode = var_251_mode_0, x = linear_16_cast_fp16)[name = tensor<string, []>("op_251_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_2_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33107072)))];
            tensor<fp16, [384]> model_encoder_layer_2_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34286784)))];
            tensor<fp16, [1, 128, 384]> linear_17_cast_fp16 = linear(bias = model_encoder_layer_2_output_dense_bias_to_fp16, weight = model_encoder_layer_2_output_dense_weight_to_fp16, x = var_251_cast_fp16)[name = tensor<string, []>("linear_17_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_31_cast_fp16 = add(x = linear_17_cast_fp16, y = var_246_cast_fp16)[name = tensor<string, []>("input_31_cast_fp16")];
            tensor<int32, [1]> var_262_axes_0 = const()[name = tensor<string, []>("op_262_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34287616)))];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34288448)))];
            tensor<fp16, [1, 128, 384]> var_262_cast_fp16 = layer_norm(axes = var_262_axes_0, beta = model_encoder_layer_2_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_2_output_LayerNorm_weight_to_fp16, x = input_31_cast_fp16)[name = tensor<string, []>("op_262_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34289280)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34584256)))];
            tensor<fp16, [1, 128, 384]> linear_18_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_query_bias_to_fp16, weight = model_encoder_layer_3_attention_self_query_weight_to_fp16, x = var_262_cast_fp16)[name = tensor<string, []>("linear_18_cast_fp16")];
            tensor<int32, [4]> var_276 = const()[name = tensor<string, []>("op_276"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_277_cast_fp16 = reshape(shape = var_276, x = linear_18_cast_fp16)[name = tensor<string, []>("op_277_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34585088)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34880064)))];
            tensor<fp16, [1, 128, 384]> linear_19_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_key_bias_to_fp16, weight = model_encoder_layer_3_attention_self_key_weight_to_fp16, x = var_262_cast_fp16)[name = tensor<string, []>("linear_19_cast_fp16")];
            tensor<int32, [4]> var_282 = const()[name = tensor<string, []>("op_282"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_283_cast_fp16 = reshape(shape = var_282, x = linear_19_cast_fp16)[name = tensor<string, []>("op_283_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34880896)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35175872)))];
            tensor<fp16, [1, 128, 384]> linear_20_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_value_bias_to_fp16, weight = model_encoder_layer_3_attention_self_value_weight_to_fp16, x = var_262_cast_fp16)[name = tensor<string, []>("linear_20_cast_fp16")];
            tensor<int32, [4]> var_288 = const()[name = tensor<string, []>("op_288"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_289_cast_fp16 = reshape(shape = var_288, x = linear_20_cast_fp16)[name = tensor<string, []>("op_289_cast_fp16")];
            tensor<int32, [4]> var_290_perm_0 = const()[name = tensor<string, []>("op_290_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_3_y_0_to_fp16 = const()[name = tensor<string, []>("mul_3_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 128, 12, 32]> mul_3_cast_fp16 = mul(x = var_277_cast_fp16, y = mul_3_y_0_to_fp16)[name = tensor<string, []>("mul_3_cast_fp16")];
            tensor<bool, []> matmul_3_transpose_y_0 = const()[name = tensor<string, []>("matmul_3_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_3_transpose_x_0 = const()[name = tensor<string, []>("matmul_3_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_30_perm_0 = const()[name = tensor<string, []>("transpose_30_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_31_perm_0 = const()[name = tensor<string, []>("transpose_31_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 128, 32]> transpose_31 = transpose(perm = transpose_31_perm_0, x = var_283_cast_fp16)[name = tensor<string, []>("transpose_45")];
            tensor<fp16, [1, 12, 128, 32]> transpose_30 = transpose(perm = transpose_30_perm_0, x = mul_3_cast_fp16)[name = tensor<string, []>("transpose_46")];
            tensor<fp16, [1, 12, 128, 128]> matmul_3_cast_fp16 = matmul(transpose_x = matmul_3_transpose_x_0, transpose_y = matmul_3_transpose_y_0, x = transpose_30, y = transpose_31)[name = tensor<string, []>("matmul_3_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_3_cast_fp16 = add(x = matmul_3_cast_fp16, y = var_73_cast_fp16)[name = tensor<string, []>("add_3_cast_fp16")];
            tensor<int32, []> softmax_3_axis_0 = const()[name = tensor<string, []>("softmax_3_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_3_cast_fp16 = softmax(axis = softmax_3_axis_0, x = add_3_cast_fp16)[name = tensor<string, []>("softmax_3_cast_fp16")];
            tensor<bool, []> var_291_transpose_x_0 = const()[name = tensor<string, []>("op_291_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> var_291_transpose_y_0 = const()[name = tensor<string, []>("op_291_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> var_290_cast_fp16 = transpose(perm = var_290_perm_0, x = var_289_cast_fp16)[name = tensor<string, []>("transpose_47")];
            tensor<fp16, [1, 12, 128, 32]> var_291_cast_fp16 = matmul(transpose_x = var_291_transpose_x_0, transpose_y = var_291_transpose_y_0, x = softmax_3_cast_fp16, y = var_290_cast_fp16)[name = tensor<string, []>("op_291_cast_fp16")];
            tensor<int32, [4]> var_292_perm_0 = const()[name = tensor<string, []>("op_292_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_293 = const()[name = tensor<string, []>("op_293"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> var_292_cast_fp16 = transpose(perm = var_292_perm_0, x = var_291_cast_fp16)[name = tensor<string, []>("transpose_44")];
            tensor<fp16, [1, 128, 384]> var_294_cast_fp16 = reshape(shape = var_293, x = var_292_cast_fp16)[name = tensor<string, []>("op_294_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35176704)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35471680)))];
            tensor<fp16, [1, 128, 384]> linear_21_cast_fp16 = linear(bias = model_encoder_layer_3_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_3_attention_output_dense_weight_to_fp16, x = var_294_cast_fp16)[name = tensor<string, []>("linear_21_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_35_cast_fp16 = add(x = linear_21_cast_fp16, y = var_262_cast_fp16)[name = tensor<string, []>("input_35_cast_fp16")];
            tensor<int32, [1]> var_305_axes_0 = const()[name = tensor<string, []>("op_305_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35472512)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35473344)))];
            tensor<fp16, [1, 128, 384]> var_305_cast_fp16 = layer_norm(axes = var_305_axes_0, beta = model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16, x = input_35_cast_fp16)[name = tensor<string, []>("op_305_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_3_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35474176)))];
            tensor<fp16, [1536]> model_encoder_layer_3_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36653888)))];
            tensor<fp16, [1, 128, 1536]> linear_22_cast_fp16 = linear(bias = model_encoder_layer_3_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_3_intermediate_dense_weight_to_fp16, x = var_305_cast_fp16)[name = tensor<string, []>("linear_22_cast_fp16")];
            tensor<string, []> var_310_mode_0 = const()[name = tensor<string, []>("op_310_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> var_310_cast_fp16 = gelu(mode = var_310_mode_0, x = linear_22_cast_fp16)[name = tensor<string, []>("op_310_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_3_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36657024)))];
            tensor<fp16, [384]> model_encoder_layer_3_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37836736)))];
            tensor<fp16, [1, 128, 384]> linear_23_cast_fp16 = linear(bias = model_encoder_layer_3_output_dense_bias_to_fp16, weight = model_encoder_layer_3_output_dense_weight_to_fp16, x = var_310_cast_fp16)[name = tensor<string, []>("linear_23_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_39_cast_fp16 = add(x = linear_23_cast_fp16, y = var_305_cast_fp16)[name = tensor<string, []>("input_39_cast_fp16")];
            tensor<int32, [1]> var_321_axes_0 = const()[name = tensor<string, []>("op_321_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37837568)))];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37838400)))];
            tensor<fp16, [1, 128, 384]> var_321_cast_fp16 = layer_norm(axes = var_321_axes_0, beta = model_encoder_layer_3_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_3_output_LayerNorm_weight_to_fp16, x = input_39_cast_fp16)[name = tensor<string, []>("op_321_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(37839232)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38134208)))];
            tensor<fp16, [1, 128, 384]> linear_24_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_query_bias_to_fp16, weight = model_encoder_layer_4_attention_self_query_weight_to_fp16, x = var_321_cast_fp16)[name = tensor<string, []>("linear_24_cast_fp16")];
            tensor<int32, [4]> var_335 = const()[name = tensor<string, []>("op_335"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_336_cast_fp16 = reshape(shape = var_335, x = linear_24_cast_fp16)[name = tensor<string, []>("op_336_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38135040)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38430016)))];
            tensor<fp16, [1, 128, 384]> linear_25_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_key_bias_to_fp16, weight = model_encoder_layer_4_attention_self_key_weight_to_fp16, x = var_321_cast_fp16)[name = tensor<string, []>("linear_25_cast_fp16")];
            tensor<int32, [4]> var_341 = const()[name = tensor<string, []>("op_341"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_342_cast_fp16 = reshape(shape = var_341, x = linear_25_cast_fp16)[name = tensor<string, []>("op_342_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38430848)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38725824)))];
            tensor<fp16, [1, 128, 384]> linear_26_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_value_bias_to_fp16, weight = model_encoder_layer_4_attention_self_value_weight_to_fp16, x = var_321_cast_fp16)[name = tensor<string, []>("linear_26_cast_fp16")];
            tensor<int32, [4]> var_347 = const()[name = tensor<string, []>("op_347"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_348_cast_fp16 = reshape(shape = var_347, x = linear_26_cast_fp16)[name = tensor<string, []>("op_348_cast_fp16")];
            tensor<int32, [4]> var_349_perm_0 = const()[name = tensor<string, []>("op_349_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_4_y_0_to_fp16 = const()[name = tensor<string, []>("mul_4_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 128, 12, 32]> mul_4_cast_fp16 = mul(x = var_336_cast_fp16, y = mul_4_y_0_to_fp16)[name = tensor<string, []>("mul_4_cast_fp16")];
            tensor<bool, []> matmul_4_transpose_y_0 = const()[name = tensor<string, []>("matmul_4_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_4_transpose_x_0 = const()[name = tensor<string, []>("matmul_4_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_32_perm_0 = const()[name = tensor<string, []>("transpose_32_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_33_perm_0 = const()[name = tensor<string, []>("transpose_33_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 128, 32]> transpose_33 = transpose(perm = transpose_33_perm_0, x = var_342_cast_fp16)[name = tensor<string, []>("transpose_41")];
            tensor<fp16, [1, 12, 128, 32]> transpose_32 = transpose(perm = transpose_32_perm_0, x = mul_4_cast_fp16)[name = tensor<string, []>("transpose_42")];
            tensor<fp16, [1, 12, 128, 128]> matmul_4_cast_fp16 = matmul(transpose_x = matmul_4_transpose_x_0, transpose_y = matmul_4_transpose_y_0, x = transpose_32, y = transpose_33)[name = tensor<string, []>("matmul_4_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_4_cast_fp16 = add(x = matmul_4_cast_fp16, y = var_73_cast_fp16)[name = tensor<string, []>("add_4_cast_fp16")];
            tensor<int32, []> softmax_4_axis_0 = const()[name = tensor<string, []>("softmax_4_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_4_cast_fp16 = softmax(axis = softmax_4_axis_0, x = add_4_cast_fp16)[name = tensor<string, []>("softmax_4_cast_fp16")];
            tensor<bool, []> var_350_transpose_x_0 = const()[name = tensor<string, []>("op_350_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> var_350_transpose_y_0 = const()[name = tensor<string, []>("op_350_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> var_349_cast_fp16 = transpose(perm = var_349_perm_0, x = var_348_cast_fp16)[name = tensor<string, []>("transpose_43")];
            tensor<fp16, [1, 12, 128, 32]> var_350_cast_fp16 = matmul(transpose_x = var_350_transpose_x_0, transpose_y = var_350_transpose_y_0, x = softmax_4_cast_fp16, y = var_349_cast_fp16)[name = tensor<string, []>("op_350_cast_fp16")];
            tensor<int32, [4]> var_351_perm_0 = const()[name = tensor<string, []>("op_351_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_352 = const()[name = tensor<string, []>("op_352"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> var_351_cast_fp16 = transpose(perm = var_351_perm_0, x = var_350_cast_fp16)[name = tensor<string, []>("transpose_40")];
            tensor<fp16, [1, 128, 384]> var_353_cast_fp16 = reshape(shape = var_352, x = var_351_cast_fp16)[name = tensor<string, []>("op_353_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38726656)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39021632)))];
            tensor<fp16, [1, 128, 384]> linear_27_cast_fp16 = linear(bias = model_encoder_layer_4_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_4_attention_output_dense_weight_to_fp16, x = var_353_cast_fp16)[name = tensor<string, []>("linear_27_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_43_cast_fp16 = add(x = linear_27_cast_fp16, y = var_321_cast_fp16)[name = tensor<string, []>("input_43_cast_fp16")];
            tensor<int32, [1]> var_364_axes_0 = const()[name = tensor<string, []>("op_364_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39022464)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39023296)))];
            tensor<fp16, [1, 128, 384]> var_364_cast_fp16 = layer_norm(axes = var_364_axes_0, beta = model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16, x = input_43_cast_fp16)[name = tensor<string, []>("op_364_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_4_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39024128)))];
            tensor<fp16, [1536]> model_encoder_layer_4_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40203840)))];
            tensor<fp16, [1, 128, 1536]> linear_28_cast_fp16 = linear(bias = model_encoder_layer_4_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_4_intermediate_dense_weight_to_fp16, x = var_364_cast_fp16)[name = tensor<string, []>("linear_28_cast_fp16")];
            tensor<string, []> var_369_mode_0 = const()[name = tensor<string, []>("op_369_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> var_369_cast_fp16 = gelu(mode = var_369_mode_0, x = linear_28_cast_fp16)[name = tensor<string, []>("op_369_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_4_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40206976)))];
            tensor<fp16, [384]> model_encoder_layer_4_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41386688)))];
            tensor<fp16, [1, 128, 384]> linear_29_cast_fp16 = linear(bias = model_encoder_layer_4_output_dense_bias_to_fp16, weight = model_encoder_layer_4_output_dense_weight_to_fp16, x = var_369_cast_fp16)[name = tensor<string, []>("linear_29_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_47_cast_fp16 = add(x = linear_29_cast_fp16, y = var_364_cast_fp16)[name = tensor<string, []>("input_47_cast_fp16")];
            tensor<int32, [1]> var_380_axes_0 = const()[name = tensor<string, []>("op_380_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41387520)))];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41388352)))];
            tensor<fp16, [1, 128, 384]> var_380_cast_fp16 = layer_norm(axes = var_380_axes_0, beta = model_encoder_layer_4_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_4_output_LayerNorm_weight_to_fp16, x = input_47_cast_fp16)[name = tensor<string, []>("op_380_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41389184)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41684160)))];
            tensor<fp16, [1, 128, 384]> linear_30_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_query_bias_to_fp16, weight = model_encoder_layer_5_attention_self_query_weight_to_fp16, x = var_380_cast_fp16)[name = tensor<string, []>("linear_30_cast_fp16")];
            tensor<int32, [4]> var_394 = const()[name = tensor<string, []>("op_394"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_395_cast_fp16 = reshape(shape = var_394, x = linear_30_cast_fp16)[name = tensor<string, []>("op_395_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41684992)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41979968)))];
            tensor<fp16, [1, 128, 384]> linear_31_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_key_bias_to_fp16, weight = model_encoder_layer_5_attention_self_key_weight_to_fp16, x = var_380_cast_fp16)[name = tensor<string, []>("linear_31_cast_fp16")];
            tensor<int32, [4]> var_400 = const()[name = tensor<string, []>("op_400"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_401_cast_fp16 = reshape(shape = var_400, x = linear_31_cast_fp16)[name = tensor<string, []>("op_401_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41980800)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42275776)))];
            tensor<fp16, [1, 128, 384]> linear_32_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_value_bias_to_fp16, weight = model_encoder_layer_5_attention_self_value_weight_to_fp16, x = var_380_cast_fp16)[name = tensor<string, []>("linear_32_cast_fp16")];
            tensor<int32, [4]> var_406 = const()[name = tensor<string, []>("op_406"), val = tensor<int32, [4]>([1, -1, 12, 32])];
            tensor<fp16, [1, 128, 12, 32]> var_407_cast_fp16 = reshape(shape = var_406, x = linear_32_cast_fp16)[name = tensor<string, []>("op_407_cast_fp16")];
            tensor<int32, [4]> var_408_perm_0 = const()[name = tensor<string, []>("op_408_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_5_y_0_to_fp16 = const()[name = tensor<string, []>("mul_5_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 128, 12, 32]> mul_5_cast_fp16 = mul(x = var_395_cast_fp16, y = mul_5_y_0_to_fp16)[name = tensor<string, []>("mul_5_cast_fp16")];
            tensor<bool, []> matmul_5_transpose_y_0 = const()[name = tensor<string, []>("matmul_5_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_5_transpose_x_0 = const()[name = tensor<string, []>("matmul_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_34_perm_0 = const()[name = tensor<string, []>("transpose_34_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_35_perm_0 = const()[name = tensor<string, []>("transpose_35_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 128, 32]> transpose_35 = transpose(perm = transpose_35_perm_0, x = var_401_cast_fp16)[name = tensor<string, []>("transpose_37")];
            tensor<fp16, [1, 12, 128, 32]> transpose_34 = transpose(perm = transpose_34_perm_0, x = mul_5_cast_fp16)[name = tensor<string, []>("transpose_38")];
            tensor<fp16, [1, 12, 128, 128]> matmul_5_cast_fp16 = matmul(transpose_x = matmul_5_transpose_x_0, transpose_y = matmul_5_transpose_y_0, x = transpose_34, y = transpose_35)[name = tensor<string, []>("matmul_5_cast_fp16")];
            tensor<fp16, [1, 12, 128, 128]> add_5_cast_fp16 = add(x = matmul_5_cast_fp16, y = var_73_cast_fp16)[name = tensor<string, []>("add_5_cast_fp16")];
            tensor<int32, []> softmax_5_axis_0 = const()[name = tensor<string, []>("softmax_5_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 128, 128]> softmax_5_cast_fp16 = softmax(axis = softmax_5_axis_0, x = add_5_cast_fp16)[name = tensor<string, []>("softmax_5_cast_fp16")];
            tensor<bool, []> var_409_transpose_x_0 = const()[name = tensor<string, []>("op_409_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> var_409_transpose_y_0 = const()[name = tensor<string, []>("op_409_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 128, 32]> var_408_cast_fp16 = transpose(perm = var_408_perm_0, x = var_407_cast_fp16)[name = tensor<string, []>("transpose_39")];
            tensor<fp16, [1, 12, 128, 32]> var_409_cast_fp16 = matmul(transpose_x = var_409_transpose_x_0, transpose_y = var_409_transpose_y_0, x = softmax_5_cast_fp16, y = var_408_cast_fp16)[name = tensor<string, []>("op_409_cast_fp16")];
            tensor<int32, [4]> var_410_perm_0 = const()[name = tensor<string, []>("op_410_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_411 = const()[name = tensor<string, []>("op_411"), val = tensor<int32, [3]>([1, 128, 384])];
            tensor<fp16, [1, 128, 12, 32]> var_410_cast_fp16 = transpose(perm = var_410_perm_0, x = var_409_cast_fp16)[name = tensor<string, []>("transpose_36")];
            tensor<fp16, [1, 128, 384]> var_412_cast_fp16 = reshape(shape = var_411, x = var_410_cast_fp16)[name = tensor<string, []>("op_412_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42276608)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42571584)))];
            tensor<fp16, [1, 128, 384]> linear_33_cast_fp16 = linear(bias = model_encoder_layer_5_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_5_attention_output_dense_weight_to_fp16, x = var_412_cast_fp16)[name = tensor<string, []>("linear_33_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_51_cast_fp16 = add(x = linear_33_cast_fp16, y = var_380_cast_fp16)[name = tensor<string, []>("input_51_cast_fp16")];
            tensor<int32, [1]> var_423_axes_0 = const()[name = tensor<string, []>("op_423_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42572416)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42573248)))];
            tensor<fp16, [1, 128, 384]> var_423_cast_fp16 = layer_norm(axes = var_423_axes_0, beta = model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16, x = input_51_cast_fp16)[name = tensor<string, []>("op_423_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_5_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42574080)))];
            tensor<fp16, [1536]> model_encoder_layer_5_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43753792)))];
            tensor<fp16, [1, 128, 1536]> linear_34_cast_fp16 = linear(bias = model_encoder_layer_5_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_5_intermediate_dense_weight_to_fp16, x = var_423_cast_fp16)[name = tensor<string, []>("linear_34_cast_fp16")];
            tensor<string, []> var_428_mode_0 = const()[name = tensor<string, []>("op_428_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 128, 1536]> var_428_cast_fp16 = gelu(mode = var_428_mode_0, x = linear_34_cast_fp16)[name = tensor<string, []>("op_428_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_5_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43756928)))];
            tensor<fp16, [384]> model_encoder_layer_5_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44936640)))];
            tensor<fp16, [1, 128, 384]> linear_35_cast_fp16 = linear(bias = model_encoder_layer_5_output_dense_bias_to_fp16, weight = model_encoder_layer_5_output_dense_weight_to_fp16, x = var_428_cast_fp16)[name = tensor<string, []>("linear_35_cast_fp16")];
            tensor<fp16, [1, 128, 384]> input_cast_fp16 = add(x = linear_35_cast_fp16, y = var_423_cast_fp16)[name = tensor<string, []>("input_cast_fp16")];
            tensor<int32, [1]> var_439_axes_0 = const()[name = tensor<string, []>("op_439_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44937472)))];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(44938304)))];
            tensor<fp16, [1, 128, 384]> var_439 = layer_norm(axes = var_439_axes_0, beta = model_encoder_layer_5_output_LayerNorm_bias_to_fp16, epsilon = var_18_to_fp16, gamma = model_encoder_layer_5_output_LayerNorm_weight_to_fp16, x = input_cast_fp16)[name = tensor<string, []>("op_439_cast_fp16")];
        } -> (var_439);
}